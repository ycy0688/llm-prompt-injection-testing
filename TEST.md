# Prompt Injection Attack Detection
é©—è­‰LLMç³»çµ±èƒ½å¦æ­£ç¢ºè­˜åˆ¥å’Œæ‹’çµ•æƒ¡æ„çš„æç¤ºæ³¨å…¥æ”»æ“Š

ğŸ” æ¸¬è©¦é¡å‹ / Test Categories:
   â€¢ è¼¸å…¥æ¨¡ç³Šæ¸¬è©¦ / Input Fuzzing
   â€¢ ç³»çµ±æŒ‡ä»¤ç¹é / System Instruction Bypassing
   â€¢ æ•¸æ“šæ´©éœ²æ¢æ¸¬ / Data Leakage Probing
   â€¢ è§’è‰²åˆ©ç”¨æ”»æ“Š / Role Exploitation

âœ… é æœŸçµæœ / Expected Results:
 å®‰å…¨çš„LLMæ‡‰è©²æ‹’çµ•æ‰€æœ‰æƒ¡æ„è«‹æ±‚ä¸¦çµ¦å‡ºé©ç•¶çš„æ‹’çµ•å›æ‡‰
 Secure LLMs should reject all malicious requests with appropriate refusal responses

### Install dependencies 
   ```bash
   pip install -r requirements.txt
   ```
### How to run
```bash
python start_testing.py
```
`start_testing.py` æ˜¯ä¸»é¸å–®è…³æœ¬ï¼Œæä¾›ç°¡å–®çš„å…¥å£ï¼Œ
å”åŠ©ä½¿ç”¨è€…å¿«é€Ÿå•Ÿå‹•å„ç¨®Prompt Injection Attackæ¸¬è©¦ã€‚

## ä¸»è¦åŠŸèƒ½
- **ç’°å¢ƒæª¢æŸ¥**ï¼šè‡ªå‹•æª¢æŸ¥å¿…è¦çš„ Python å¥—ä»¶èˆ‡ `.env` æª”æ¡ˆï¼Œç¢ºä¿æ¸¬è©¦ç’°å¢ƒæ­£ç¢ºã€‚
- **ä¸»é¸å–®å°è¦½**ï¼šä»¥é¸å–®æ–¹å¼æä¾›å¤šç¨®æ¸¬è©¦é¸é …ï¼ŒåŒ…æ‹¬ï¼š
  - View Attack Examplesï¼šåˆ—å‡ºå¸¸è¦‹æ”»æ“Š promptã€‚
  - Interactive Testingï¼šæ‰‹å‹•è¼¸å…¥ prompt é€²è¡Œæ¸¬è©¦ã€‚
  - Integrate with Existing Frameworkï¼š èˆ‡å¤–éƒ¨ç¨‹å¼`main.py`çµåˆã€‚
  - Exitã€‚
- **è‡ªå‹•åŸ·è¡Œæ¸¬è©¦è…³æœ¬**ï¼šæ ¹æ“šé¸æ“‡è‡ªå‹•å‘¼å«å°æ‡‰çš„æ¸¬è©¦è…³æœ¬ã€‚

å°ˆæ¡ˆä¸­çš„ `main.py` æª”æ¡ˆæ²¿ç”¨è‡ª[https://github.com/greshake/llm-security/tree/main]ï¼Œ
å·²æ•´åˆæ–¼ä¸»é¸å–®è…³æœ¬ï¼Œä¹Ÿå¯ç¨ç«‹åŸ·è¡Œæ¸¬è©¦ï¼š
```bash
python main.py
```
è©³ç´° package ç”¨æ³•è«‹åƒè€ƒåŸä½œè€…çš„ README.mdã€‚

---
æœ¬æª”æ¡ˆè¨­è¨ˆç°¡å–®æ˜“ç”¨ï¼Œé©åˆå„ç¨® LLM å®‰å…¨æ¸¬è©¦å ´æ™¯ï¼Œå¾ŒçºŒå¯ä¾éœ€æ±‚æ“´å……æˆ–æ•´åˆå…¶ä»–æ¸¬è©¦è…³æœ¬ã€‚
