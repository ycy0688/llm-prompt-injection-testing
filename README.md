# Prompt Injection Attack Detection
é©—è­‰LLMç³»çµ±èƒ½å¦æ­£ç¢ºè­˜åˆ¥å’Œæ‹’çµ•æƒ¡æ„çš„æç¤ºæ³¨å…¥æ”»æ“Š

ğŸ” æ¸¬è©¦é¡å‹ / Test Categories:
   â€¢ è¼¸å…¥æ¨¡ç³Šæ¸¬è©¦ / Input Fuzzing
   â€¢ ç³»çµ±æŒ‡ä»¤ç¹é / System Instruction Bypassing
   â€¢ æ•¸æ“šæ´©éœ²æ¢æ¸¬ / Data Leakage Probing
   â€¢ è§’è‰²åˆ©ç”¨æ”»æ“Š / Role Exploitation

âœ… é æœŸçµæœ / Expected Results:
 å®‰å…¨çš„LLMæ‡‰è©²æ‹’çµ•æ‰€æœ‰æƒ¡æ„è«‹æ±‚ä¸¦çµ¦å‡ºé©ç•¶çš„æ‹’çµ•å›æ‡‰
 Secure LLMs should reject all malicious requests with appropriate refusal responses

### Install dependencies 
   ```bash
   pip install -r requirements.txt
   ```
### How to run
```bash
python start_testing.py
```
`start_testing.py` æ˜¯ä¸»é¸å–®è…³æœ¬ï¼Œæä¾›ç°¡å–®çš„å…¥å£ï¼Œ
å”åŠ©ä½¿ç”¨è€…å¿«é€Ÿå•Ÿå‹•å„ç¨®Prompt Injection Attackæ¸¬è©¦ã€‚

## ä¸»è¦åŠŸèƒ½
- **ç’°å¢ƒæª¢æŸ¥**ï¼šè‡ªå‹•æª¢æŸ¥å¿…è¦çš„ Python å¥—ä»¶èˆ‡ `.env` æª”æ¡ˆï¼Œç¢ºä¿æ¸¬è©¦ç’°å¢ƒæ­£ç¢ºã€‚

**ä¸»é¸å–®å°è¦½**ï¼šä»¥é¸å–®æ–¹å¼æä¾›å¤šç¨®æ¸¬è©¦é¸é …ï¼ŒåŒ…æ‹¬ï¼š
\- View Attack Examplesï¼šåˆ—å‡ºå¸¸è¦‹æ”»æ“Š promptã€‚
\- Interactive Testingï¼šæ‰‹å‹•è¼¸å…¥ prompt é€²è¡Œæ¸¬è©¦ã€‚
\- Integrate with Existing Frameworkï¼š èˆ‡å¤–éƒ¨ç¨‹å¼`main.py`çµåˆã€‚
\- Exitã€‚
  
## æ•´åˆæ–¼æœ¬å°ˆæ¡ˆçš„ä¸»é¸å–®è…³æœ¬ä¸­çš„2è·Ÿ3ï¼Œä¹Ÿå¯ä»¥ç¨ç«‹åŸ·è¡Œæ¸¬è©¦ï¼š
`demo_prompt_injection.py`ç”¨æ–¼äº’å‹•æ¸¬è©¦æ¨¡å‹æ˜¯å¦èƒ½æ­£ç¢ºè­˜åˆ¥ä¸¦æ‹’çµ•æç¤ºæ³¨å…¥æ”»æ“Šã€‚

åŸ·è¡Œï¼š
```bash
python demo_prompt_injection.py
```

 `main.py` æª”æ¡ˆä¾†æºæ–¼é–‹æºå°ˆæ¡ˆ [llm-security](https://github.com/greshake/llm-security/tree/main)ï¼Œæ­¤å°ˆæ¡ˆä¸­æä¾›äº†å„ç¨®æ”»æ“Šåª’ä»‹å’ŒæŠ€è¡“çš„ç¤ºç¯„ã€‚

```bash
python main.py
```

**é—œæ–¼ main.py çš„è©³ç´°åŠŸèƒ½èˆ‡ç”¨æ³•ï¼Œè«‹åƒè€ƒåŸä½œè€…çš„å®Œæ•´èªªæ˜æ–‡ä»¶ï¼š**
- ğŸ”— [GitHub Repository](https://github.com/greshake/llm-security)

---
æœ¬æª”æ¡ˆè¨­è¨ˆç°¡å–®æ˜“ç”¨ï¼Œé©åˆå„ç¨® LLM å®‰å…¨æ¸¬è©¦å ´æ™¯ï¼Œå¾ŒçºŒå¯ä¾éœ€æ±‚æ“´å……æˆ–æ•´åˆå…¶ä»–æ¸¬è©¦è…³æœ¬ã€‚
